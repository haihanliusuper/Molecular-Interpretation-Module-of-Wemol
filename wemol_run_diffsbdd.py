#!/opt/conda/envs/diffsbdd4/bin/python
# -*- coding: utf-8 -*-

import argparse
import subprocess
import sys
from pathlib import Path
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, MolToSmiles
from Bio.PDB import PDBParser, PDBIO, is_aa
import numpy as np

CWD = Path.cwd()
ROOT = Path(__file__).resolve().parent
CHECKPOINTS_DIR = ROOT / "checkpoints"
WATERS = {"HOH", "WAT", "TIP3"}
MIN_CARBON_COUNT = 3


def run_command(cmd):
    print("â–¶ Running:", " ".join(map(str, cmd)))
    subprocess.run(cmd, check=True)


def get_min_atom_distance_and_indices(m1, m2):
    conf1, conf2 = m1.GetConformer(), m2.GetConformer()
    d_min, i1_min, i2_min = float("inf"), -1, -1
    for a1 in m1.GetAtoms():
        p1 = np.array(conf1.GetAtomPosition(a1.GetIdx()))
        for a2 in m2.GetAtoms():
            p2 = np.array(conf2.GetAtomPosition(a2.GetIdx()))
            d = np.linalg.norm(p1 - p2)
            if d < d_min:
                d_min, i1_min, i2_min = d, a1.GetIdx(), a2.GetIdx()
    return d_min, i1_min, i2_min


def estimate_min_carbons_from_sdf(sdf_path):
    mol = Chem.SDMolSupplier(str(sdf_path), removeHs=False)[0]
    frags = Chem.GetMolFrags(mol, asMols=True, sanitizeFrags=True)
    if len(frags) != 2:
        raise ValueError(f"Expected 2 fragments in SDF, but found {len(frags)}")
    m1, m2 = frags
    if not m1.GetNumConformers():
        AllChem.EmbedMolecule(m1, randomSeed=42)
    if not m2.GetNumConformers():
        AllChem.EmbedMolecule(m2, randomSeed=42)
    dist, idx1, idx2 = get_min_atom_distance_and_indices(m1, m2)
    n_carbon = max(1, round(dist / 1.5))
    print(f"Estimated connecting carbon count: {n_carbon}")
    return n_carbon


def remove_hydrogens_from_sdf(sdf_path, output_path):
    supplier = Chem.SDMolSupplier(str(sdf_path), removeHs=False)
    writer = Chem.SDWriter(str(output_path))
    for mol in supplier:
        if mol is not None:
            mol_no_h = Chem.RemoveHs(mol)
            writer.write(mol_no_h)
    writer.close()


def process_optimize_csv(sdf_out_path):
    csv_path = Path(sdf_out_path).with_suffix('.csv')
    if csv_path.exists():
        try:
            df = pd.read_csv(csv_path)
            df = df[['score', 'smiles']]
            df.columns = ['Score', 'SMILES']
            df.to_csv(csv_path, index=False)
        except Exception as e:
            print(f"CSV processing failed: {e}")


def split_complex(complex_pdb, protein_out, ligand_pdb_out, ligand_sdf_out, min_atoms=5, max_atoms=200):
    parser = PDBParser(QUIET=True)
    struct = parser.get_structure("complex", str(complex_pdb))
    ligand_structure = struct.copy()
    for model in ligand_structure:
        for chain in list(model):
            model.detach_child(chain.id)

    ligand_count = 0
    for model in struct:
        for chain in model:
            new_chain = chain.copy()
            for res in list(new_chain):
                new_chain.detach_child(res.id)
            has_lig = False
            for res in chain:
                if is_aa(res) or res.resname.strip() in WATERS:
                    continue
                if not (min_atoms <= len(res) <= max_atoms):
                    continue
                carbon_count = sum(1 for atom in res.get_atoms() if atom.element == "C")
                if carbon_count < MIN_CARBON_COUNT:
                    continue
                new_chain.add(res.copy())
                has_lig, ligand_count = True, ligand_count + 1
            if has_lig:
                ligand_structure[0].add(new_chain)
    if ligand_count == 0:
        raise RuntimeError("No suitable ligand residues found")
    io = PDBIO()
    io.set_structure(ligand_structure)
    io.save(str(ligand_pdb_out))
    mol = Chem.MolFromPDBFile(str(ligand_pdb_out), sanitize=True, removeHs=False)
    if not mol:
        raise RuntimeError("RDKit failed to read ligand PDB")
    w = Chem.SDWriter(str(ligand_sdf_out))
    w.write(mol)
    w.close()

    protein_structure = struct.copy()
    for model in protein_structure:
        for chain in list(model):
            for res in list(chain):
                if not (is_aa(res) or res.resname.strip() in WATERS):
                    chain.detach_child(res.id)
    io.set_structure(protein_structure)
    io.save(str(protein_out))


def merge_and_deduplicate_sdfs(sdf_list, output_path):
    seen = set()
    writer = Chem.SDWriter(str(output_path))
    mol_idx = 1

    for sdf in sdf_list:
        for mol in Chem.SDMolSupplier(str(sdf), removeHs=False):
            if mol is None:
                continue

            frags = Chem.GetMolFrags(mol, asMols=True, sanitizeFrags=True)
            if len(frags) > 1:
                biggest = max(frags, key=lambda m: m.GetNumAtoms())
                mol = biggest
                print(f"âš ï¸ Found {len(frags)} fragments, keeping largest one ({mol.GetNumAtoms()} atoms).")

            try:
                smiles = MolToSmiles(mol)
            except:
                print("âš ï¸ SMILES generation failed, skipping molecule.")
                continue

            if smiles not in seen:
                seen.add(smiles)
                mol.SetProp("_Name", f"Mol_{mol_idx}")
                writer.write(mol)
                mol_idx += 1

    writer.close()
    print(f"ğŸ§¬ Final cleaned, merged and deduplicated molecules saved to: {output_path}")

def merge_optimize_csvs(generated_sdf_files, output_csv_path, objective="sa"):

    records = []
    for sdf_file in generated_sdf_files:
        csv_path = Path(sdf_file).with_suffix(".csv")
        if not csv_path.exists():
            print(f"âš ï¸ CSV not found for {sdf_file}, skip merging this one.")
            continue
        try:
            df = pd.read_csv(csv_path)
            # å®¹é”™ï¼šç»Ÿä¸€åˆ—å
            if set(df.columns) >= {"score", "smiles"}:
                df = df.rename(columns={"score": "Score", "smiles": "SMILES"})
            elif set(df.columns) >= {"Score", "SMILES"}:
                pass
            else:
                print(f"âš ï¸ Unexpected CSV columns in {csv_path}: {list(df.columns)}; skipping.")
                continue

            # è®°å½•æ‰¹æ¬¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            df["BatchFile"] = Path(sdf_file).name
            records.append(df[["Score", "SMILES", "BatchFile"]])
        except Exception as e:
            print(f"âš ï¸ Failed to read {csv_path}: {e}")

    if not records:
        print("âš ï¸ No CSV files to merge.")
        return

    big = pd.concat(records, axis=0, ignore_index=True)

    # å»é‡ç­–ç•¥ï¼šæŒ‰ SMILES åˆ†ç»„ï¼Œä¿ç•™æ›´ä¼˜åˆ†æ•°
    if objective.lower() == "qed":
        # åˆ†æ•°è¶Šå¤§è¶Šå¥½
        big = big.sort_values(["SMILES", "Score"], ascending=[True, False])
    else:
        # SA è¶Šå°è¶Šå¥½ï¼ˆæˆ–å…¶ä»–é»˜è®¤è¶Šå°è¶Šå¥½ï¼‰
        big = big.sort_values(["SMILES", "Score"], ascending=[True, True])

    big = big.drop_duplicates(subset=["SMILES"], keep="first")

    # æœ€ç»ˆæ’åºï¼šä¾¿äºäººå·¥æŸ¥çœ‹
    if objective.lower() == "qed":
        big = big.sort_values("Score", ascending=False)
    else:
        big = big.sort_values("Score", ascending=True)

    big.to_csv(output_csv_path, index=False)
    print(f"ğŸ§¾ Merged CSV saved to: {output_csv_path}")
def _canonical_smiles(smiles: str) -> str:
    try:
        m = Chem.MolFromSmiles(smiles)
        return Chem.MolToSmiles(m, canonical=True) if m else smiles
    except:
        return smiles

def _build_smiles_to_mol_map(sdf_files):
    mapping = {}
    for sdf in sdf_files:
        supp = Chem.SDMolSupplier(str(sdf), removeHs=False)
        for mol in supp:
            if mol is None:
                continue
            try:
                smi = _canonical_smiles(Chem.MolToSmiles(mol))
            except:
                continue
            # åªä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„ç‰ˆæœ¬å³å¯ï¼ˆé¿å…é‡å¤è¦†ç›–ï¼‰
            if smi not in mapping:
                mapping[smi] = mol
    return mapping

def select_top_n_from_merged(merged_csv_path, sdf_sources, out_csv_path, out_sdf_path, n=100, objective="qed"):
    """
    æ ¹æ®åˆå¹¶å¥½çš„ CSVï¼ˆåˆ—ï¼šScore, SMILESï¼‰ï¼Œé€‰å‡º Top-Nï¼ˆqed è¶Šå¤§è¶Šå¥½ï¼›sa è¶Šå°è¶Šå¥½ï¼‰ï¼Œ
    å†™å‡ºä¸€ä¸ªç²¾ç®€ç‰ˆ CSV + å¯¹åº” SDFï¼ˆä»ç»™å®šçš„ SDF æºä¸­æŒ‰ SMILES ç²¾å‡†å–å‡ºï¼›è‹¥ç¼ºå¤±åˆ™ fallback ç”¨ SMILES ç°å»º 3D æ„è±¡ï¼‰ã€‚
    """
    merged_csv_path = Path(merged_csv_path)
    if not merged_csv_path.exists():
        print(f"âŒ Merged CSV not found: {merged_csv_path}")
        return

    try:
        df = pd.read_csv(merged_csv_path)
    except Exception as e:
        print(f"âŒ Failed to read merged CSV: {e}")
        return

    # ç»Ÿä¸€åˆ—å
    cols = {c.lower(): c for c in df.columns}
    if "score" in cols and "smiles" in cols:
        df = df.rename(columns={cols["score"]: "Score", cols["smiles"]: "SMILES"})
    else:
        print(f"âŒ Unexpected merged CSV columns: {list(df.columns)}")
        return

    # å»é‡ï¼ˆå…ˆæŒ‰ç›®æ ‡æ’åºï¼Œå† drop_duplicates ä¿ç•™æœ€ä½³ï¼‰
    if str(objective).lower() == "qed":
        df = df.sort_values(["SMILES", "Score"], ascending=[True, False])
        df = df.drop_duplicates(subset=["SMILES"], keep="first")
        df = df.sort_values("Score", ascending=False)
    else:
        # é»˜è®¤è®¤ä¸ºåˆ†æ•°è¶Šå°è¶Šå¥½ï¼ˆä¾‹å¦‚ SAï¼‰
        df = df.sort_values(["SMILES", "Score"], ascending=[True, True])
        df = df.drop_duplicates(subset=["SMILES"], keep="first")
        df = df.sort_values("Score", ascending=True)

    # å–å‰ n æ¡
    df_top = df.head(int(n)).copy()

    # å†™ CSV
    out_csv_path = Path(out_csv_path)
    out_csv_path.parent.mkdir(parents=True, exist_ok=True)
    df_top.to_csv(out_csv_path, index=False)
    print(f"ğŸ† Top-{n} CSV saved to: {out_csv_path}")

    # ä¸ºäº†å†™ SDFï¼Œå…ˆæ„å»º SMILES->Mol çš„æ£€ç´¢è¡¨
    # ä¼˜å…ˆä½¿ç”¨åˆå¹¶åçš„æ€» SDFï¼›è‹¥æ²¡æœ‰ï¼Œå°±æ‹¼æ‰€æœ‰æ‰¹æ¬¡çš„ SDF
    sdf_sources = [Path(p) for p in sdf_sources if Path(p).exists()]
    smiles2mol = _build_smiles_to_mol_map(sdf_sources)

    writer = Chem.SDWriter(str(out_sdf_path))
    kept = 0
    for _, row in df_top.iterrows():
        smi = _canonical_smiles(row["SMILES"])
        mol = smiles2mol.get(smi, None)
        if mol is None:
            # fallbackï¼šç”¨ SMILES é‡æ–°æ„å»ºä¸€ä¸ªåˆ†å­ï¼Œè¡¥ä¸€ä¸ª 3D æ„è±¡ï¼Œä¿è¯ SDF èƒ½å†™å‡ºæ¥
            m = Chem.MolFromSmiles(smi)
            if m is None:
                print(f"âš ï¸ Cannot rebuild from SMILES: {smi}")
                continue
            m = Chem.AddHs(m)
            try:
                AllChem.EmbedMolecule(m, randomSeed=42)
                AllChem.UFFOptimizeMolecule(m, maxIters=200)
            except Exception as e:
                print(f"âš ï¸ Embed/opt failed for {smi}: {e}")
            m.SetProp("_Name", f"Top_{kept+1}")
            m.SetProp("Score", str(row["Score"]))
            writer.write(m)
            kept += 1
            continue

        # ä»æº SDF æ‹¿åˆ°çš„ molï¼Œè¡¥å……ä¸€ä¸‹å±æ€§å†å†™
        try:
            mol.SetProp("_Name", mol.GetProp("_Name") if mol.HasProp("_Name") else f"Top_{kept+1}")
        except:
            mol.SetProp("_Name", f"Top_{kept+1}")
        mol.SetProp("Score", str(row["Score"]))
        writer.write(mol)
        kept += 1

    writer.close()
    print(f"ğŸ§ª Top-{n} SDF saved to: {out_sdf_path} (kept {kept} molecules)")


def main():
    parser = argparse.ArgumentParser("Unified DiffSBDD Runner")
    parser.add_argument("--mode", choices=["design", "inpaint", "optimize", "inpaint_auto"], required=True)
    parser.add_argument("--pdb_complex", required=True)
    parser.add_argument("--checkpoint", default=str(CHECKPOINTS_DIR / "crossdocked_fullatom_cond.ckpt"))
    parser.add_argument("--outfile")
    parser.add_argument("--n_samples", type=int, default=20)
    parser.add_argument("--center", choices=["ligand", "pocket"], default="ligand")
    parser.add_argument("--add_n_nodes", type=int, default=10)
    parser.add_argument("--objective", choices=["sa", "qed"], default="sa")
    parser.add_argument("--population_size", type=int, default=20)
    parser.add_argument("--evolution_steps", type=int, default=2)
    parser.add_argument("--top_k", type=int, default=10)
    parser.add_argument("--timesteps", type=int, default=100)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    complex_path = Path(args.pdb_complex).resolve()
    protein_pdb = CWD / "protein_only.pdb"
    ligand_pdb = CWD / "ligand_only.pdb"
    ligand_sdf = CWD / "ligand_only.sdf"
    ligand_sdf_noH = CWD / "ligand_only_noH.sdf"

    print("ğŸ” Splitting complex ...")
    split_complex(complex_path, protein_pdb, ligand_pdb, ligand_sdf)
    print("ğŸ§¼ Removing hydrogens ...")
    remove_hydrogens_from_sdf(ligand_sdf, ligand_sdf_noH)

    generated_files = []

    # ---------------------------
    # æ‰¹æ¬¡æ‰§è¡Œå™¨
    # ---------------------------
    def build_and_run(base_cmd, output_prefix):
        if args.mode == "optimize":
            n = args.n_samples
            batch_size = args.population_size
            chunks = (n + batch_size - 1) // batch_size
            for i in range(chunks):
                print(f"ğŸš€ Running optimize batch {i + 1}/{chunks} ...")
                batch_seed = (args.seed or 0) + i
                out_file = Path(f"{output_prefix}_{i + 1}.sdf")
                cmd = base_cmd + ["--outfile", str(out_file), "--seed", str(batch_seed)]
                run_command(cmd)
                process_optimize_csv(out_file)  # è§„èŒƒåˆ—åä¸º Score/SMILES
                generated_files.append(out_file)
        else:
            # design/inpaintï¼šæ¯æ‰¹æœ€å¤š 50 ä¸ªï¼Œå¹¶åš 1.2x å†—ä½™
            chunks = max(1, (args.n_samples + 49) // 50)
            redundant_total = int(np.ceil(args.n_samples * 1.2))
            for i in range(chunks):
                n = min(50, redundant_total - i * 50)
                if n <= 0:
                    break
                print(f"ğŸš€ Generating batch {i + 1}/{chunks} with {n} molecules ...")
                batch_seed = (args.seed or 0) + i
                out_file = Path(f"{output_prefix}_{i + 1}.sdf")
                cmd = base_cmd + ["--n_samples", str(n), "--outfile", str(out_file), "--seed", str(batch_seed)]
                run_command(cmd)
                generated_files.append(out_file)

    # ---------------------------
    # ç»„è£…æŒ‡ä»¤å¹¶è¿è¡Œ
    # ---------------------------
    if args.mode == "design":
        base_cmd = [
            sys.executable, str(ROOT / "generate_ligands.py"),
            str(args.checkpoint),
            "--pdbfile", str(protein_pdb),
            "--ref_ligand", str(ligand_sdf_noH),
        ]
        build_and_run(base_cmd, "design")

    elif args.mode == "inpaint":
        base_cmd = [
            sys.executable, str(ROOT / "inpaint.py"),
            str(args.checkpoint),
            "--pdbfile", str(protein_pdb),
            "--ref_ligand", str(ligand_sdf_noH),
            "--fix_atoms", str(ligand_sdf_noH),
            "--add_n_nodes", str(args.add_n_nodes),
        ]
        build_and_run(base_cmd, "inpaint")

    elif args.mode == "optimize":
        base_cmd = [
            sys.executable, str(ROOT / "optimize2.py"),
            "--checkpoint", args.checkpoint,
            "--pdbfile", str(protein_pdb),
            "--ref_ligand", str(ligand_sdf_noH),
            "--objective", args.objective,
            "--population_size", str(args.population_size),
            "--evolution_steps", str(args.evolution_steps),
            "--top_k", str(args.top_k),
            "--timesteps", str(args.timesteps),
        ]
        build_and_run(base_cmd, "optimize")

    elif args.mode == "inpaint_auto":
        min_carbons = estimate_min_carbons_from_sdf(ligand_sdf)
        total_samples = args.n_samples
        max_per_batch = 50
        redundant_total = int(np.ceil(total_samples * 1.2))
        total_batches = max(1, (redundant_total + max_per_batch - 1) // max_per_batch)

        for j, add_nodes in enumerate(range(min_carbons + 4, min_carbons + 10), 1):
            print(f"ğŸ” add_n_nodes = {add_nodes}, generating {total_samples} molecules in {total_batches} batches ...")
            for i in range(total_batches):
                n = min(max_per_batch, redundant_total - i * max_per_batch)
                if n <= 0:
                    break
                out = CWD / f"inpaint_auto_{j}_{i + 1}.sdf"
                batch_seed = (args.seed or 0) + j * 100 + i
                cmd = [
                    sys.executable, str(ROOT / "inpaint.py"),
                    str(args.checkpoint),
                    "--pdbfile", str(protein_pdb),
                    "--outfile", str(out),
                    "--ref_ligand", str(ligand_sdf_noH),
                    "--fix_atoms", str(ligand_sdf_noH),
                    "--add_n_nodes", str(add_nodes),
                    "--n_samples", str(n),
                    "--seed", str(batch_seed),
                ]
                print(f"ğŸš€ Generating with add_n_nodes={add_nodes}, batch {i + 1}/{total_batches}, n={n}")
                run_command(cmd)
                generated_files.append(out)

    # ---------------------------
    # åˆå¹¶ & é€‰ Top-N
    # ---------------------------
    if generated_files:
        # é optimizeï¼šä»…åš SDF åˆå¹¶/å»é‡ï¼ˆè‹¥æä¾› --outfileï¼‰ï¼Œä¸åš CSV/Top-N
        if args.mode != "optimize":
            if args.outfile:
                print(f"ğŸ“¦ (non-optimize) Merging and deduplicating {len(generated_files)} SDFs into: {args.outfile}")
                merge_and_deduplicate_sdfs(generated_files, args.outfile)
            else:
                print("â„¹ï¸ (non-optimize) No --outfile provided; keep batch SDFs as-is. Skipping CSV merge and Top-N.")
            return

        # ========= ä»¥ä¸‹ä»… optimize =========
        # åˆå¹¶ SDFï¼ˆå¯é€‰ï¼‰å¹¶ç¡®å®š CSV/SDF æ¥æº
        if args.outfile:
            print(f"ğŸ“¦ Merging and deduplicating {len(generated_files)} files (SDF) ...")
            merge_and_deduplicate_sdfs(generated_files, args.outfile)
            merged_csv_path = Path(args.outfile).with_suffix(".csv")
            sdf_sources = [args.outfile]  # Top-N ä¼˜å…ˆä»åˆå¹¶åçš„ SDF å–
        else:
            merged_csv_path = CWD / "optimize_merged.csv"
            sdf_sources = generated_files

        # åˆå¹¶æ‰¹æ¬¡ CSV
        print("ğŸ“Š Merging batch CSV files into one ...")
        merge_optimize_csvs(generated_files, merged_csv_path, objective=args.objective)

        # åŸºäºåˆå¹¶ CSV é€‰ Top-Nï¼ˆè‹¥å‰ä¸€æ­¥æ²¡äº§å‡º CSVï¼Œä¼šåœ¨å‡½æ•°é‡Œä¼˜é›…é€€å‡ºï¼‰
        top_n = int(args.n_samples)
        if args.outfile:
            top_csv_out = Path(args.outfile).with_name(f"{Path(args.outfile).stem}_top{top_n}.csv")
            top_sdf_out = Path(args.outfile).with_name(f"{Path(args.outfile).stem}_top{top_n}.sdf")
        else:
            stem = f"{args.mode}_top{top_n}"
            top_csv_out = CWD / f"{stem}.csv"
            top_sdf_out = CWD / f"{stem}.sdf"

        print(f"ğŸ Selecting Top-{top_n} molecules by {args.objective} ...")
        select_top_n_from_merged(
            merged_csv_path=merged_csv_path,
            sdf_sources=sdf_sources,
            out_csv_path=top_csv_out,
            out_sdf_path=top_sdf_out,
            n=top_n,
            objective=args.objective
        )


if __name__ == "__main__":
    main()
